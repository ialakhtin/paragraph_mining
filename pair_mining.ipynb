{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "866ae902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class Validator:\n",
    "    def __init__(self, threshold=0.5, src_dir='data/validator'):\n",
    "        with open(os.path.join(src_dir, 'ru_w2l.json'), encoding='utf-8') as f:\n",
    "            self.ru_lemmas = json.load(f)\n",
    "        with open(os.path.join(src_dir, 'en_w2l.json'), encoding='utf-8') as f:\n",
    "            self.en_lemmas = json.load(f)\n",
    "        with open(os.path.join(src_dir, 'ru_en.json'), encoding='utf-8') as f:\n",
    "            self.ru_en = json.load(f)\n",
    "        with open(os.path.join(src_dir, 'en_ru.json'), encoding='utf-8') as f:\n",
    "            self.en_ru = json.load(f)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def lemmatize(self, sent, lang, lemmas):\n",
    "        tokens = word_tokenize(sent.strip().lower(), language=lang)\n",
    "        \n",
    "        return [lemmas[token] for token in tokens if token in lemmas]\n",
    "\n",
    "    def get_score_ordered(self, src_tokens, dst_tokens, mapper):\n",
    "        if len(dst_tokens) == 0:\n",
    "            return 0\n",
    "        trans_tokens = [mapper[token] for token in src_tokens if token in mapper]\n",
    "        dst_set = set(dst_tokens)\n",
    "        trans_set = set(trans_tokens)\n",
    "        score = len(dst_set & trans_set) / max(len(dst_set), len(trans_set))\n",
    "        return score\n",
    "\n",
    "    def get_score(self, en_sent, ru_sent):\n",
    "        en_tokens = self.lemmatize(en_sent, 'english', self.en_lemmas)\n",
    "        ru_tokens = self.lemmatize(ru_sent, 'russian', self.ru_lemmas)\n",
    "        return (\n",
    "            self.get_score_ordered(en_tokens, ru_tokens, self.en_ru) +\n",
    "            self.get_score_ordered(ru_tokens, en_tokens, self.ru_en)\n",
    "        ) / 2\n",
    "\n",
    "    def fit(self, en_sents, ru_sents):\n",
    "        random_labels = np.random.choice(len(en_sents), (len(en_sents), 2))\n",
    "        bad_scores = []\n",
    "        for en, ru in random_labels:\n",
    "            score = self.get_score(en_sents[en], ru_sents[ru])\n",
    "            bad_scores.append(score)\n",
    "        self.threshold = np.percentile(bad_scores, 95)\n",
    "\n",
    "    def validate(self, en_sent, ru_sent):\n",
    "        return self.get_score(en_sent, ru_sent) > self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1cf623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LangDataset(Dataset):\n",
    "    def __init__(self, emb):\n",
    "        self.emb = emb\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'emb': self.emb[index], \n",
    "            'index': index\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c0f8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def df_to_tuples(df):\n",
    "    mapper = lambda x: x if type(x) != torch.Tensor else x.item()\n",
    "    transform = lambda data: list(map(mapper, data))\n",
    "    pairs = list(zip(transform(df['en'].to_list()), transform(df['ru'].to_list())))\n",
    "    pairs = list(set(pairs))\n",
    "    return pairs\n",
    "\n",
    "def get_labse_encoder(model):\n",
    "    def encode(data, lang):\n",
    "        return model.encode(data, show_progress_bar=False)\n",
    "    return encode\n",
    "\n",
    "def get_laser_encoder(en_model, ru_model):\n",
    "    models = {\n",
    "        'en': en_model,\n",
    "        'ru': ru_model,\n",
    "    }\n",
    "    def encode(data, lang):\n",
    "        return models[lang].encode_sentences(data)\n",
    "    return encode\n",
    "\n",
    "class SentenceMiner:\n",
    "    def __init__(self, en_sents, ru_sents, encoder, use_margin=False, batch_size=32):\n",
    "        self.sents = {\n",
    "            'en': en_sents,\n",
    "            'ru': ru_sents,\n",
    "        }\n",
    "        print(\"Encoding\")\n",
    "        self.emb = {\n",
    "            'en': encoder(en_sents, 'en'),\n",
    "            'ru': encoder(ru_sents, 'ru'),\n",
    "        }\n",
    "        self.emb_dim = self.emb['en'].shape[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.use_margin = use_margin\n",
    "        if use_margin:\n",
    "            self.margins = {\n",
    "                'en': np.zeros(len(en_sents)),\n",
    "                'ru': np.zeros(len(ru_sents)),\n",
    "            }\n",
    "            self.nfind = 5\n",
    "        self.index = {}\n",
    "        print(\"Initialized\")\n",
    "\n",
    "    def build_index(self, lang):\n",
    "        print(f\"{lang} index building\")\n",
    "        M = 32\n",
    "        nlist = 16384\n",
    "        nprobe = 32\n",
    "\n",
    "        opq = faiss.OPQMatrix(self.emb_dim, M)\n",
    "\n",
    "        quantizer = faiss.IndexFlatL2(self.emb_dim)\n",
    "        index_ivfpq = faiss.IndexIVFPQ(quantizer, self.emb_dim, nlist, M, 8)\n",
    "        index_ivfpq.nprobe = nprobe\n",
    "        index_ivfpq.metric_type = faiss.METRIC_INNER_PRODUCT\n",
    "\n",
    "        index = faiss.IndexPreTransform(opq, index_ivfpq)\n",
    "        res = faiss.StandardGpuResources()\n",
    "        gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        gpu_index.train(self.emb[lang])\n",
    "        gpu_index.add(self.emb[lang])\n",
    "        print(f\"index trained\")\n",
    "        return gpu_index\n",
    "\n",
    "    def find_pairs_ordered(self, lang_src, lang_dst):\n",
    "        index = self.build_index(lang_dst)\n",
    "        src_ds = LangDataset(self.emb[lang_src])\n",
    "        src_dataloader = DataLoader(src_ds, batch_size=self.batch_size)\n",
    "        pairs = []\n",
    "        for batch in src_dataloader:\n",
    "            D, I = index.search(batch['emb'], self.nfind)\n",
    "            for src_i, dst_i, d in zip(batch['index'], I, D):\n",
    "                if self.use_margin:\n",
    "                    self.margins[lang_src][src_i] = np.mean(d)\n",
    "                pairs.append((src_i.item(), dst_i[0], d[0]))\n",
    "        return pd.DataFrame(pairs, columns=[lang_src, lang_dst, 'score'])\n",
    "\n",
    "    def find_pairs(self):\n",
    "        df = pd.concat([\n",
    "            self.find_pairs_ordered('en', 'ru'),\n",
    "            self.find_pairs_ordered('ru', 'en'),\n",
    "        ])\n",
    "        if self.use_margin:\n",
    "            new_scores = []\n",
    "            for _, row in df.iterrows():\n",
    "                en = int(row['en'])\n",
    "                ru = int(row['ru'])\n",
    "                score = 2 * row['score'] / (\n",
    "                    self.margins['en'][en] +\n",
    "                    self.margins['ru'][ru]\n",
    "                )\n",
    "                new_scores.append((en, ru, score))\n",
    "            df = pd.DataFrame(new_scores, columns=['en', 'ru', 'score'])\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dfd9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, labels):\n",
    "    n_true = len(labels)\n",
    "    n_pred = len(preds)\n",
    "    TP = len(set(labels) & set(preds))\n",
    "    precision = TP / n_pred\n",
    "    recall = TP / n_true\n",
    "    f0_5 = (1 + 1/4) * TP / (n_true / 4 + n_pred)\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F0.5: {f0_5:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c128e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'data/miner_test'\n",
    "\n",
    "with open(os.path.join(test_dir, 'en_sents'), 'r') as f:\n",
    "    en_sents = f.readlines()\n",
    "    en_sents = [sent.strip() for sent in en_sents]\n",
    "\n",
    "with open(os.path.join(test_dir, 'ru_sents'), 'r') as f:\n",
    "    ru_sents = f.readlines()\n",
    "    ru_sents = [sent.strip() for sent in ru_sents]\n",
    "\n",
    "labels_df = pd.read_csv(os.path.join(test_dir, 'labels.csv'))\n",
    "label_pairs = df_to_tuples(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5797a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_score(df, threshold=0.65, drop_rate=0.1):\n",
    "    df = df[df['score'] > threshold]\n",
    "    df = df.sort_values(by='score')\n",
    "    return df[int(len(df) * drop_rate):]\n",
    "\n",
    "def filter_by_validator(pairs, threshold=0.1, drop_rate=0):\n",
    "    pairs_with_score = []\n",
    "    validator = Validator()\n",
    "    for en, ru in pairs:\n",
    "        score = validator.get_score(en_sents[en], ru_sents[ru])\n",
    "        if score > threshold:\n",
    "            pairs_with_score.append((en, ru, score))\n",
    "    pairs_with_score = sorted(pairs_with_score, key=lambda x: x[2])[int(len(pairs) * drop_rate):]\n",
    "    pairs = list(map(lambda x: (x[0], x[1]), pairs_with_score))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8551721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/paragraph_mining/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_base = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "lora_model = torch.load('lora_labse2', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7c089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 22202/22202 [04:31<00:00, 81.83it/s] \n",
      "Batches: 100%|██████████| 22202/22202 [04:50<00:00, 76.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "ru index building\n",
      "index trained\n",
      "en index building\n",
      "index trained\n"
     ]
    }
   ],
   "source": [
    "# LaBSE\n",
    "miner = SentenceMiner(en_sents, ru_sents, get_labse_encoder(model_base))\n",
    "preds_df = miner.find_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc9699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.881\n",
      "Recall: 0.773\n",
      "F0.5: 0.857\n"
     ]
    }
   ],
   "source": [
    "preds_df = preds_df.drop_duplicates(subset=['en', 'ru'])\n",
    "preds = df_to_tuples(filter_by_score(preds_df))\n",
    "preds = filter_by_validator(preds)\n",
    "evaluate(preds, label_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d7ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 22202/22202 [07:56<00:00, 46.59it/s]\n",
      "Batches: 100%|██████████| 22202/22202 [08:01<00:00, 46.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "ru index building\n",
      "index trained\n",
      "en index building\n",
      "index trained\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned\n",
    "miner = SentenceMiner(en_sents, ru_sents, get_labse_encoder(lora_model))\n",
    "preds_df = miner.find_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a3b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.179\n",
      "Recall: 0.500\n",
      "F0.5: 0.205\n"
     ]
    }
   ],
   "source": [
    "preds_df = preds_df.drop_duplicates(subset=['en', 'ru'])\n",
    "preds = df_to_tuples(filter_by_score(preds_df))\n",
    "preds = filter_by_validator(preds)\n",
    "evaluate(preds, label_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b75e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 22202/22202 [04:32<00:00, 81.47it/s] \n",
      "Batches: 100%|██████████| 22202/22202 [04:48<00:00, 77.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "ru index building\n",
      "index trained\n",
      "en index building\n",
      "index trained\n"
     ]
    }
   ],
   "source": [
    "# LaBSE + margin\n",
    "miner = SentenceMiner(en_sents, ru_sents, get_labse_encoder(model_base), use_margin=True)\n",
    "preds_df = miner.find_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26f1d406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.946\n",
      "Recall: 0.888\n",
      "F0.5: 0.934\n"
     ]
    }
   ],
   "source": [
    "preds_df = preds_df.drop_duplicates(subset=['en', 'ru'])\n",
    "preds = df_to_tuples(filter_by_score(preds_df, threshold=1.06))\n",
    "preds = filter_by_validator(preds)\n",
    "evaluate(preds, label_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 22202/22202 [08:04<00:00, 45.87it/s]\n",
      "Batches: 100%|██████████| 22202/22202 [08:25<00:00, 43.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "ru index building\n",
      "index trained\n",
      "en index building\n",
      "index trained\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned + margin\n",
    "miner = SentenceMiner(en_sents, ru_sents, get_labse_encoder(lora_model), use_margin=True)\n",
    "preds_df = miner.find_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97cfc522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.991\n",
      "Recall: 0.040\n",
      "F0.5: 0.171\n"
     ]
    }
   ],
   "source": [
    "preds_df = preds_df.drop_duplicates(subset=['en', 'ru'])\n",
    "preds = df_to_tuples(filter_by_score(preds_df, threshold=1.06))\n",
    "preds = filter_by_validator(preds)\n",
    "evaluate(preds, label_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc261765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-30 13:42:50,826 | INFO | laser_encoders.download_models |  - laser2.spm already downloaded\n",
      "2025-07-30 13:42:50,902 | INFO | laser_encoders.download_models |  - laser2.pt already downloaded\n",
      "2025-07-30 13:42:50,904 | INFO | laser_encoders.download_models |  - laser2.spm already downloaded\n",
      "2025-07-30 13:42:50,905 | INFO | laser_encoders.download_models |  - laser2.cvocab already downloaded\n",
      "2025-07-30 13:42:51,949 | INFO | laser_encoders.download_models |  - laser2.spm already downloaded\n",
      "2025-07-30 13:42:52,012 | INFO | laser_encoders.download_models |  - laser2.pt already downloaded\n",
      "2025-07-30 13:42:52,013 | INFO | laser_encoders.download_models |  - laser2.spm already downloaded\n",
      "2025-07-30 13:42:52,014 | INFO | laser_encoders.download_models |  - laser2.cvocab already downloaded\n"
     ]
    }
   ],
   "source": [
    "from laser_encoders import LaserEncoderPipeline\n",
    "en_encoder = LaserEncoderPipeline(lang=\"eng_Latn\")\n",
    "ru_encoder = LaserEncoderPipeline(lang=\"rus_Cyrl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc5352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding\n"
     ]
    }
   ],
   "source": [
    "# LASER\n",
    "miner = SentenceMiner(en_sents, ru_sents, get_laser_encoder(en_encoder, ru_encoder), use_margin=True)\n",
    "preds_df = miner.find_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = preds_df.drop_duplicates(subset=['en', 'ru'])\n",
    "preds = df_to_tuples(filter_by_score(preds_df, threshold=1.06))\n",
    "preds = filter_by_validator(preds)\n",
    "evaluate(preds, label_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "with open('YOUR/PATH/EN', 'r') as f:\n",
    "    en_sents = f.readlines()\n",
    "\n",
    "with open('YOUR/PATH/RU', 'r') as f:\n",
    "    ru_sents = f.readlines()\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "miner = SentenceMiner(en_sents, ru_sents, get_labse_encoder(model_base), use_margin=True)\n",
    "\n",
    "preds_df = miner.find_pairs().drop_duplicates(subset=['en', 'ru'])\n",
    "preds = df_to_tuples(filter_by_score(preds_df, threshold=1.06))\n",
    "preds = filter_by_validator(preds)\n",
    "\n",
    "sent_pairs = [(en_sents[en], ru_sents[ru]) for en, ru in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace4282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
